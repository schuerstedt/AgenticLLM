# AgenticLLM TODO

- [x] write a node.js terminal llm browser which allows to connect to a localhost openai compatible llm (which does not exist so test will fail). Simple text based chat via terminal.
- [x] write a open ai llm echo proxy which echoes all input and listens on localhost (which would make Step 1 work)
- [ ] use the local gemini in headless mode instead of the echo functionality https://geminicli.com/docs/cli/headless/